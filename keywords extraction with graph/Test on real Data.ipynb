{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import re \n",
    "import operator\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from library import clean_text_simple,terms_to_graph,core_dec,accuracy_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stpwds = stopwords.words('english')\n",
    "punct = string.punctuation.replace('-', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 files processed\n",
      "100 files processed\n",
      "200 files processed\n",
      "300 files processed\n",
      "400 files processed\n",
      "0 abstracts processed\n",
      "100 abstracts processed\n",
      "200 abstracts processed\n",
      "300 abstracts processed\n",
      "400 abstracts processed\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "# read and pre-process abstracts #\n",
    "##################################\n",
    "\n",
    "path_to_abstracts = \"data/Hulth2003testing/abstracts\"\n",
    "abstract_names = sorted(os.listdir(path_to_abstracts))\n",
    "\n",
    "abstracts = []\n",
    "for counter,filename in enumerate(abstract_names):\n",
    "    # read file\n",
    "    with open(path_to_abstracts + '/' + filename, 'r') as my_file: \n",
    "        text = my_file.read().splitlines()\n",
    "    text = ' '.join(text)\n",
    "    # remove formatting\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    abstracts.append(text)\n",
    "    \n",
    "    if counter % round(len(abstract_names)/5) == 0:\n",
    "        print(counter, 'files processed')\n",
    "\n",
    "abstracts_cleaned = []\n",
    "for counter,abstract in enumerate(abstracts):\n",
    "    my_tokens = clean_text_simple(abstract,my_stopwords=stpwds,punct=punct)\n",
    "    abstracts_cleaned.append(my_tokens)\n",
    "    \n",
    "    if counter % round(len(abstracts)/5) == 0:\n",
    "        print(counter, 'abstracts processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 files processed\n",
      "100 files processed\n",
      "200 files processed\n",
      "300 files processed\n",
      "400 files processed\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "# read and pre-process gold standard keywords #\n",
    "###############################################\n",
    "\n",
    "path_to_keywords = \"data/Hulth2003testing/uncontr\"\n",
    "keywd_names = sorted(os.listdir(path_to_keywords))\n",
    "   \n",
    "keywds_gold_standard = []\n",
    "\n",
    "for counter,filename in enumerate(keywd_names):\n",
    "    # read file\n",
    "    with open(path_to_keywords +'/'+ filename, 'r') as my_file: \n",
    "        text = my_file.read().splitlines()\n",
    "    text = ' '.join(text)\n",
    "    text =  re.sub('\\s+', ' ', text) # remove formatting\n",
    "    text = text.lower()\n",
    "    # turn string into list of keywords, preserving intra-word dashes \n",
    "    # but breaking n-grams into unigrams\n",
    "    keywds = text.split(';')\n",
    "    keywds = [keywd.strip().split(' ') for keywd in keywds]\n",
    "    keywds = [keywd for sublist in keywds for keywd in sublist] # flatten list\n",
    "    keywds = [keywd for keywd in keywds if keywd not in stpwds] # remove stopwords (rare but may happen due to n-gram breaking)\n",
    "    keywds_stemmed = [stemmer.stem(keywd) for keywd in keywds]\n",
    "    keywds_stemmed_unique = list(set(keywds_stemmed)) # remove duplicates (may happen due to n-gram breaking)\n",
    "    keywds_gold_standard.append(keywds_stemmed_unique)\n",
    "    \n",
    "    if counter % round(len(keywd_names)/5) == 0:\n",
    "        print(counter, 'files processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# precompute graphs-of-words #\n",
    "##############################\n",
    "\n",
    "gs = []\n",
    "for extract in abstracts_cleaned:\n",
    "    gs.append(terms_to_graph(extract,11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "# graph-based keyword extraction #\n",
    "##################################\n",
    "\n",
    "my_percentage = 0.33 # for PR and TF-IDF\n",
    "\n",
    "method_names = ['kc','wkc','pr','tfidf']\n",
    "keywords = dict(zip(method_names,[[],[],[],[]]))\n",
    "\n",
    "for counter,g in enumerate(gs):\n",
    "    # k-core\n",
    "    core_numbers = core_dec(g,False)\n",
    "    core_numbers = [i[0] for i in sorted(core_numbers.items() , key=lambda t : t[1],reverse=True) if i[1]==max(core_numbers.values())]\n",
    "    keywords['kc'].append(core_numbers[:int(len(core_numbers))])\n",
    "    \n",
    "    # weighted k-core\n",
    "    weighted_core_numbers = core_dec(g,True)\n",
    "    weighted_core_numbers = [i[0] for i in sorted(weighted_core_numbers.items() , key=lambda t : t[1],reverse=True) if i[1]==max(weighted_core_numbers.values())]\n",
    "    keywords['wkc'].append(weighted_core_numbers[:int(len(weighted_core_numbers))])\n",
    "    \n",
    "    # PageRank\n",
    "    pr_scores = zip(g.vs['name'],g.pagerank())\n",
    "    pr_scores = sorted(pr_scores, key=operator.itemgetter(1), reverse=True) # in decreasing order\n",
    "    numb_to_retain = int(len(pr_scores)*my_percentage) # retain top 'my_percentage' % words as keywords\n",
    "    keywords['pr'].append([tuple[0] for tuple in pr_scores[:numb_to_retain]])\n",
    "        \n",
    "    if counter % round(len(gs)/5) == 0:\n",
    "        print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# TF-IDF keyword extraction #\n",
    "#############################\n",
    "\n",
    "abstracts_cleaned_strings = [' '.join(elt) for elt in abstracts_cleaned] # to ensure same pre-processing as the other methods\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stpwds)\n",
    "doc_term_matrix = tfidf_vectorizer.fit_transform(abstracts_cleaned_strings)\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "vectors_list = doc_term_matrix.todense().tolist()\n",
    "\n",
    "for counter,vector in enumerate(vectors_list):\n",
    "    terms_weights = list(zip(terms,vector)) # bow feature vector as list of tuples\n",
    "    nonzero = [terms_weights[i] for i,j in enumerate(vector) if j!=0 ]\n",
    "    nonzero = sorted(nonzero, key=operator.itemgetter(1), reverse=True) # in decreasing order\n",
    "    numb_to_retain = int(len(nonzero)*my_percentage) # retain top 'my_percentage' % words as keywords\n",
    "    keywords['tfidf'].append([tuple[0] for tuple in nonzero[:numb_to_retain]])\n",
    "    \n",
    "    if counter % round(len(vectors_list)/5) == 0:\n",
    "        print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kc performance: \n",
      "\n",
      "precision: 50.44\n",
      "recall: 64.64\n",
      "F-1 score: 53.67\n",
      "\n",
      "\n",
      "wkc performance: \n",
      "\n",
      "precision: 62.82\n",
      "recall: 49.24\n",
      "F-1 score: 48.66\n",
      "\n",
      "\n",
      "pr performance: \n",
      "\n",
      "precision: 54.36\n",
      "recall: 34.69\n",
      "F-1 score: 40.7\n",
      "\n",
      "\n",
      "tfidf performance: \n",
      "\n",
      "precision: 59.21\n",
      "recall: 38.5\n",
      "F-1 score: 44.85\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "# performance comparison #\n",
    "##########################\n",
    "\n",
    "perf = dict(zip(method_names,[[],[],[],[]]))\n",
    "\n",
    "for idx,truth in enumerate(keywds_gold_standard):\n",
    "    for mn in method_names:\n",
    "        perf[mn].append(accuracy_metrics(keywords[mn][idx],truth))\n",
    "\n",
    "lkgs = len(keywds_gold_standard)\n",
    "\n",
    "for k,v in perf.items():\n",
    "    print(k + ' performance: \\n')\n",
    "    print('precision:', round(100*sum([tuple[0] for tuple in v])/lkgs,2))\n",
    "    print('recall:', round(100*sum([tuple[1] for tuple in v])/lkgs,2))\n",
    "    print('F-1 score:', round(100*sum([tuple[2] for tuple in v])/lkgs,2))\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
